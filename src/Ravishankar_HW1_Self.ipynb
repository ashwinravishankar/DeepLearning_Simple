{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from random import random\n",
    "import numpy as np\n",
    "import math\n",
    "from math import exp\n",
    "trainFileName = \"train.csv\"\n",
    "testFileName = \"test.csv\"\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDatasetFromCSV(fileName):\n",
    "    Data = []\n",
    "    with open(fileName, 'r') as file:\n",
    "        csvReader = reader(file)\n",
    "        next(csvReader, None)\n",
    "        for row in csvReader:\n",
    "            for i in range(1, 3):\n",
    "                row[i] = float(row[i].strip())\n",
    "            row[0] = int(row[0].strip())\n",
    "            Data.append(row)\n",
    "    # trainData = trainData[1:]\n",
    "    return Data\n",
    "\n",
    "#     firstRowFromCSV = trainData[0][1:]\n",
    "#     firstRowFromCSVOutput = trainData[0][0]\n",
    "def shuffleData(data):  \n",
    "    np.random.shuffle(data)\n",
    "    shuffleData = []\n",
    "    shuffledOutput = []\n",
    "    for row in data:\n",
    "        shuffleData.append(row[1:])\n",
    "        shuffledOutput.append(row[0])\n",
    "    return shuffleData, shuffledOutput\n",
    "\n",
    "# print(firstRowFromCSV)\n",
    "# print(firstRowFromCSVOutput)\n",
    "\n",
    "# for i in range(5):\n",
    "#     print(trainingData[i], trainingOutput[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNetwork():\n",
    "    numInputs = 2\n",
    "    numHidden1 = 3\n",
    "    numHidden2 = 3\n",
    "    numOutput = 2\n",
    "    hiddenLayer1 = [{'w': [random() for i in range(numInputs)]} for i in range(numHidden1)]\n",
    "    # hiddenLayer1.append({'layer': 'hidden'})\n",
    "    hiddenLayer2 = [{'w': [random() for i in range(numHidden1)]} for i in range(numHidden2)]\n",
    "    # hiddenLayer2.append({'layer': 'hidden'})\n",
    "    outputLayer = [{'w': [random() for i in range(numHidden2)]} for i in range(numOutput)]\n",
    "    # outputLayer.append({'layer': 'output'})\n",
    "    neuralNetwork = [hiddenLayer1, hiddenLayer2, outputLayer]\n",
    "    return neuralNetwork\n",
    "    x=[]\n",
    "    W1 = []\n",
    "    W2 = []\n",
    "    W3 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotProduct(W, I):\n",
    "    a=0.0\n",
    "    for i in range(len(W)):\n",
    "       a+=W[i]*I[i]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxActivationFunction(inputs):\n",
    "    sum=0\n",
    "    retVal=[]\n",
    "    for i in range(len(inputs)):\n",
    "        sum+= exp(inputs[i])\n",
    "    retVal.append(exp(inputs[0]) / sum)\n",
    "    retVal.append(exp(inputs[1]) / sum)\n",
    "    return retVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigActivationFunction(val):\n",
    "    value = 1.0 / (1.0 + exp(-val))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(val):\n",
    "    return val * (1.0 - val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropogation(network, data):\n",
    "#     print(network)\n",
    "    currentData = data\n",
    "    layerCount=0\n",
    "    softmaxInputs=[]\n",
    "    for layer in network:\n",
    "        layerCount+=1\n",
    "        activatedValues = []\n",
    "        for node in layer:\n",
    "#             print(node['weights'])\n",
    "            prod = dotProduct(node['w'], currentData)\n",
    "#             print(prod)\n",
    "            if(layerCount!=3):\n",
    "                node['o'] = sigActivationFunction(prod)\n",
    "#                 print(node['output'])\n",
    "                activatedValues.append(node['o'])\n",
    "            elif(layerCount==3):\n",
    "                softmaxInputs.append(prod)\n",
    "#                 print(\"Softmax:\", softmaxInputs)\n",
    "                if(len(softmaxInputs) == 2):\n",
    "                    activatedValues = softmaxActivationFunction(softmaxInputs)\n",
    "#                     print(\"activated: \", activatedValues)\n",
    "                    layer[0]['o'] = activatedValues[0]\n",
    "                    node['o'] = activatedValues[1]\n",
    "        currentData = activatedValues\n",
    "#     print(\"Summ Current Data: \", currentData[0] + currentData[1])\n",
    "    return currentData, network\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runForwardPropogation(nn, inputData, rowOutput):\n",
    "    fwdOutput, network = forwardPropogation(nn, inputData)\n",
    "#     print([trainData[0][:1], fwdOutput])\n",
    "    loss = 0\n",
    "    targetOutput=[]\n",
    "    if(rowOutput == 1):\n",
    "        targetOutput = [0, 1]\n",
    "    elif (rowOutput == 0):\n",
    "        targetOutput = [1, 0]\n",
    "    for i in range(2):\n",
    "        loss += pow((targetOutput[i] - fwdOutput[i]), 2)\n",
    "    return fwdOutput, loss, targetOutput, network\n",
    "    print(loss)\n",
    "# print(nn)\n",
    "#backwardPropogation(nn, targetOutput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardPropogation(network, targetOutput):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        error=[]\n",
    "        if i == len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                e = 2 * (targetOutput[j] - neuron['o'])\n",
    "                error.append(e)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                err=0.0\n",
    "                for neuron in network[i+1]:\n",
    "                    err+=(neuron['w'][j] * neuron['e'])\n",
    "                error.append(err)\n",
    "        for j in range(len(layer)):\n",
    "            neuron=layer[j]\n",
    "            neuron['e'] = error[j] * derivative(neuron['o'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBackwardPropogation(nn, targetOutput, inputData, learningRate):\n",
    "    nn = backwardPropogation(nn, targetOutput)\n",
    "#     for layer in nn:\n",
    "#         print(layer, '\\n\\n')\n",
    "    for i in range(len(nn)):\n",
    "        inputVal=inputData\n",
    "        if i!=0:\n",
    "            inputVal = [neuron['o'] for neuron in nn[i-1]]\n",
    "        for neuron in nn[i]:\n",
    "            for j in range(len(inputVal)):\n",
    "                neuron['w'][j] += learningRate * neuron['e'] * inputVal[j]\n",
    "    return nn\n",
    "# for layer in nn:\n",
    "#     print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->epoch= 1 / 1000   loss= 0.5017205197821376\n",
      "-->epoch= 2 / 1000   loss= 0.5005038613569429\n",
      "-->epoch= 3 / 1000   loss= 0.4988197110172719\n",
      "-->epoch= 4 / 1000   loss= 0.4989492996711302\n",
      "-->epoch= 5 / 1000   loss= 0.496659723674879\n",
      "-->epoch= 6 / 1000   loss= 0.4895313816864121\n",
      "-->epoch= 7 / 1000   loss= 0.46638166090524097\n",
      "-->epoch= 8 / 1000   loss= 0.3797163057619195\n",
      "-->epoch= 9 / 1000   loss= 0.26271784463740716\n",
      "-->epoch= 10 / 1000   loss= 0.2079311348833831\n",
      "-->epoch= 11 / 1000   loss= 0.1832148979832957\n",
      "-->epoch= 12 / 1000   loss= 0.17223268192169705\n",
      "-->epoch= 13 / 1000   loss= 0.1677254153981936\n",
      "-->epoch= 14 / 1000   loss= 0.16538499857107786\n",
      "-->epoch= 15 / 1000   loss= 0.1644735496131923\n",
      "-->epoch= 16 / 1000   loss= 0.163791621209553\n",
      "-->epoch= 17 / 1000   loss= 0.16351832664680457\n",
      "-->epoch= 18 / 1000   loss= 0.16314744643894807\n",
      "-->epoch= 19 / 1000   loss= 0.16305318262336627\n",
      "-->epoch= 20 / 1000   loss= 0.16304503368041964\n",
      "-->epoch= 21 / 1000   loss= 0.162950357724782\n",
      "-->epoch= 22 / 1000   loss= 0.16283650528033608\n",
      "-->epoch= 23 / 1000   loss= 0.16266119033396745\n",
      "-->epoch= 24 / 1000   loss= 0.1628325597841996\n",
      "-->epoch= 25 / 1000   loss= 0.16246362554496707\n",
      "-->epoch= 26 / 1000   loss= 0.16278133155174715\n",
      "-->epoch= 27 / 1000   loss= 0.16265255518172086\n",
      "-->epoch= 28 / 1000   loss= 0.16246259272541927\n",
      "-->epoch= 29 / 1000   loss= 0.1627369513389701\n",
      "-->epoch= 30 / 1000   loss= 0.1626590493837406\n",
      "-->epoch= 31 / 1000   loss= 0.16241119509636742\n",
      "-->epoch= 32 / 1000   loss= 0.1624764007614631\n",
      "-->epoch= 33 / 1000   loss= 0.16229079233388313\n",
      "-->epoch= 34 / 1000   loss= 0.16236382832468285\n",
      "-->epoch= 35 / 1000   loss= 0.16244940782087883\n",
      "-->epoch= 36 / 1000   loss= 0.1625099812263226\n",
      "-->epoch= 37 / 1000   loss= 0.16250235186302486\n",
      "-->epoch= 38 / 1000   loss= 0.16202677613777122\n",
      "-->epoch= 39 / 1000   loss= 0.16228879059480814\n",
      "-->epoch= 40 / 1000   loss= 0.16226782464216533\n",
      "-->epoch= 41 / 1000   loss= 0.16230434647524922\n",
      "-->epoch= 42 / 1000   loss= 0.16205479349796537\n",
      "-->epoch= 43 / 1000   loss= 0.16229085079501315\n",
      "-->epoch= 44 / 1000   loss= 0.1622587384815535\n",
      "-->epoch= 45 / 1000   loss= 0.16217896252198322\n",
      "-->epoch= 46 / 1000   loss= 0.16208977673520386\n",
      "-->epoch= 47 / 1000   loss= 0.1622263200475203\n",
      "-->epoch= 48 / 1000   loss= 0.1622040297973662\n",
      "-->epoch= 49 / 1000   loss= 0.16219330937480791\n",
      "-->epoch= 50 / 1000   loss= 0.162114306214165\n",
      "-->epoch= 51 / 1000   loss= 0.1621477509460596\n",
      "-->epoch= 52 / 1000   loss= 0.1621367265038672\n",
      "-->epoch= 53 / 1000   loss= 0.16172175707412711\n",
      "-->epoch= 54 / 1000   loss= 0.16206088018319317\n",
      "-->epoch= 55 / 1000   loss= 0.16211902633097333\n",
      "-->epoch= 56 / 1000   loss= 0.16204467019510937\n",
      "-->epoch= 57 / 1000   loss= 0.16206594983216724\n",
      "-->epoch= 58 / 1000   loss= 0.1620524999791693\n",
      "-->epoch= 59 / 1000   loss= 0.16200706032219775\n",
      "-->epoch= 60 / 1000   loss= 0.16167842308698208\n",
      "-->epoch= 61 / 1000   loss= 0.16200422442147228\n",
      "-->epoch= 62 / 1000   loss= 0.16186121686906663\n",
      "-->epoch= 63 / 1000   loss= 0.1618951865258932\n",
      "-->epoch= 64 / 1000   loss= 0.16154845562537062\n",
      "-->epoch= 65 / 1000   loss= 0.16201320813852804\n",
      "-->epoch= 66 / 1000   loss= 0.16193588037918402\n",
      "-->epoch= 67 / 1000   loss= 0.16204278028653432\n",
      "-->epoch= 68 / 1000   loss= 0.16205998907301755\n",
      "-->epoch= 69 / 1000   loss= 0.16195503162692074\n",
      "-->epoch= 70 / 1000   loss= 0.1617724230977889\n",
      "-->epoch= 71 / 1000   loss= 0.1619684636109591\n",
      "-->epoch= 72 / 1000   loss= 0.16191121376068232\n",
      "-->epoch= 73 / 1000   loss= 0.1616945173838672\n",
      "-->epoch= 74 / 1000   loss= 0.16176451151093216\n",
      "-->epoch= 75 / 1000   loss= 0.1619082313177193\n",
      "-->epoch= 76 / 1000   loss= 0.16153252130282686\n",
      "-->epoch= 77 / 1000   loss= 0.16180307221110918\n",
      "-->epoch= 78 / 1000   loss= 0.16136639925724233\n",
      "-->epoch= 79 / 1000   loss= 0.16191399964731493\n",
      "-->epoch= 80 / 1000   loss= 0.16165590944485594\n",
      "-->epoch= 81 / 1000   loss= 0.1618253695742582\n",
      "-->epoch= 82 / 1000   loss= 0.1616281755304527\n",
      "-->epoch= 83 / 1000   loss= 0.16183026822381044\n",
      "-->epoch= 84 / 1000   loss= 0.16158067250951513\n",
      "-->epoch= 85 / 1000   loss= 0.16187767260472147\n",
      "-->epoch= 86 / 1000   loss= 0.16178974687559042\n",
      "-->epoch= 87 / 1000   loss= 0.1618671527234331\n",
      "-->epoch= 88 / 1000   loss= 0.16170002044495776\n",
      "-->epoch= 89 / 1000   loss= 0.1615431199531857\n",
      "-->epoch= 90 / 1000   loss= 0.16160986843383418\n",
      "-->epoch= 91 / 1000   loss= 0.16188220061828404\n",
      "-->epoch= 92 / 1000   loss= 0.16182053203540497\n",
      "-->epoch= 93 / 1000   loss= 0.16172443081079071\n",
      "-->epoch= 94 / 1000   loss= 0.16148726025532334\n",
      "-->epoch= 95 / 1000   loss= 0.16163804812912924\n",
      "-->epoch= 96 / 1000   loss= 0.161685732681169\n",
      "-->epoch= 97 / 1000   loss= 0.16161127748058704\n",
      "-->epoch= 98 / 1000   loss= 0.1617347959124657\n",
      "-->epoch= 99 / 1000   loss= 0.1616188145922971\n",
      "-->epoch= 100 / 1000   loss= 0.16149097989574598\n",
      "-->epoch= 101 / 1000   loss= 0.16150912190185585\n",
      "-->epoch= 102 / 1000   loss= 0.16178420258303505\n",
      "-->epoch= 103 / 1000   loss= 0.16156554239600282\n",
      "-->epoch= 104 / 1000   loss= 0.161720860396613\n",
      "-->epoch= 105 / 1000   loss= 0.16168021219262838\n",
      "-->epoch= 106 / 1000   loss= 0.16150072566651236\n",
      "-->epoch= 107 / 1000   loss= 0.16163579136814502\n",
      "-->epoch= 108 / 1000   loss= 0.16161952345529815\n",
      "-->epoch= 109 / 1000   loss= 0.16170302048464563\n",
      "-->epoch= 110 / 1000   loss= 0.16156601607459284\n",
      "-->epoch= 111 / 1000   loss= 0.16153890845035507\n",
      "-->epoch= 112 / 1000   loss= 0.16154031971423632\n",
      "-->epoch= 113 / 1000   loss= 0.16134787839147188\n",
      "-->epoch= 114 / 1000   loss= 0.161591415434249\n",
      "-->epoch= 115 / 1000   loss= 0.16160726092173994\n",
      "-->epoch= 116 / 1000   loss= 0.16150819231593794\n",
      "-->epoch= 117 / 1000   loss= 0.161594344733308\n",
      "-->epoch= 118 / 1000   loss= 0.161558115843455\n",
      "-->epoch= 119 / 1000   loss= 0.16162846046580107\n",
      "-->epoch= 120 / 1000   loss= 0.16146172079170684\n",
      "-->epoch= 121 / 1000   loss= 0.161613208735141\n",
      "-->epoch= 122 / 1000   loss= 0.16176164757516234\n",
      "-->epoch= 123 / 1000   loss= 0.16162741577589423\n",
      "-->epoch= 124 / 1000   loss= 0.16169159253093132\n",
      "-->epoch= 125 / 1000   loss= 0.1609716130718912\n",
      "-->epoch= 126 / 1000   loss= 0.16147884747928778\n",
      "-->epoch= 127 / 1000   loss= 0.16140585933867002\n",
      "-->epoch= 128 / 1000   loss= 0.161434881404723\n",
      "-->epoch= 129 / 1000   loss= 0.1616034783242506\n",
      "-->epoch= 130 / 1000   loss= 0.16155186247506498\n",
      "-->epoch= 131 / 1000   loss= 0.16143959224225285\n",
      "-->epoch= 132 / 1000   loss= 0.16159503114103807\n",
      "-->epoch= 133 / 1000   loss= 0.16153576803682804\n",
      "-->epoch= 134 / 1000   loss= 0.1614258554418\n",
      "-->epoch= 135 / 1000   loss= 0.16168257234063682\n",
      "-->epoch= 136 / 1000   loss= 0.1615084663994322\n",
      "-->epoch= 137 / 1000   loss= 0.16133533554665141\n",
      "-->epoch= 138 / 1000   loss= 0.1616570644286896\n",
      "-->epoch= 139 / 1000   loss= 0.16132130345987322\n",
      "-->epoch= 140 / 1000   loss= 0.16155446591222455\n",
      "-->epoch= 141 / 1000   loss= 0.16139230838120894\n",
      "-->epoch= 142 / 1000   loss= 0.16156680881487775\n",
      "-->epoch= 143 / 1000   loss= 0.16162064805550183\n",
      "-->epoch= 144 / 1000   loss= 0.16159444834222478\n",
      "-->epoch= 145 / 1000   loss= 0.16149348022209725\n",
      "-->epoch= 146 / 1000   loss= 0.16142711510746335\n",
      "-->epoch= 147 / 1000   loss= 0.16156869666026766\n",
      "-->epoch= 148 / 1000   loss= 0.16160272031672474\n",
      "-->epoch= 149 / 1000   loss= 0.1614799068134564\n",
      "-->epoch= 150 / 1000   loss= 0.16146294030593522\n",
      "-->epoch= 151 / 1000   loss= 0.1615831889426339\n",
      "-->epoch= 152 / 1000   loss= 0.16143181469573073\n",
      "-->epoch= 153 / 1000   loss= 0.16097789107309557\n",
      "-->epoch= 154 / 1000   loss= 0.16140246923463614\n",
      "-->epoch= 155 / 1000   loss= 0.16142928978264984\n",
      "-->epoch= 156 / 1000   loss= 0.16149978424668096\n",
      "-->epoch= 157 / 1000   loss= 0.16135674292208865\n",
      "-->epoch= 158 / 1000   loss= 0.16116836607820578\n",
      "-->epoch= 159 / 1000   loss= 0.16148516973002994\n",
      "-->epoch= 160 / 1000   loss= 0.16109896430333334\n",
      "-->epoch= 161 / 1000   loss= 0.16156692208214676\n",
      "-->epoch= 162 / 1000   loss= 0.16164807062374736\n",
      "-->epoch= 163 / 1000   loss= 0.16144077680419802\n",
      "-->epoch= 164 / 1000   loss= 0.1613711366155542\n",
      "-->epoch= 165 / 1000   loss= 0.16146737163531757\n",
      "-->epoch= 166 / 1000   loss= 0.16159307071041126\n",
      "-->epoch= 167 / 1000   loss= 0.1614053334269278\n",
      "-->epoch= 168 / 1000   loss= 0.16123023638807854\n",
      "-->epoch= 169 / 1000   loss= 0.160855821313944\n",
      "-->epoch= 170 / 1000   loss= 0.16133900941012336\n",
      "-->epoch= 171 / 1000   loss= 0.16134012291084637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->epoch= 172 / 1000   loss= 0.16176506049206535\n",
      "-->epoch= 173 / 1000   loss= 0.16151506157426204\n",
      "-->epoch= 174 / 1000   loss= 0.16135391313814704\n",
      "-->epoch= 175 / 1000   loss= 0.1614219895550789\n",
      "-->epoch= 176 / 1000   loss= 0.16134928643703914\n",
      "-->epoch= 177 / 1000   loss= 0.16129736131291147\n",
      "-->epoch= 178 / 1000   loss= 0.16088133975845134\n",
      "-->epoch= 179 / 1000   loss= 0.16110492840643495\n",
      "-->epoch= 180 / 1000   loss= 0.1611498443441318\n",
      "-->epoch= 181 / 1000   loss= 0.16113722021748483\n",
      "-->epoch= 182 / 1000   loss= 0.16137945672312465\n",
      "-->epoch= 183 / 1000   loss= 0.16095819986123558\n",
      "-->epoch= 184 / 1000   loss= 0.16142657690431048\n",
      "-->epoch= 185 / 1000   loss= 0.16145511948929012\n",
      "-->epoch= 186 / 1000   loss= 0.16153337708729637\n",
      "-->epoch= 187 / 1000   loss= 0.16119283029915238\n",
      "-->epoch= 188 / 1000   loss= 0.16145482074838416\n",
      "-->epoch= 189 / 1000   loss= 0.16155052932973077\n",
      "-->epoch= 190 / 1000   loss= 0.1615183164367849\n",
      "-->epoch= 191 / 1000   loss= 0.1612162420112788\n",
      "-->epoch= 192 / 1000   loss= 0.16135468763037428\n",
      "-->epoch= 193 / 1000   loss= 0.16085237518688622\n",
      "-->epoch= 194 / 1000   loss= 0.16161902174247333\n",
      "-->epoch= 195 / 1000   loss= 0.16133370872555017\n",
      "-->epoch= 196 / 1000   loss= 0.16114798257625454\n",
      "-->epoch= 197 / 1000   loss= 0.16146264801217397\n",
      "-->epoch= 198 / 1000   loss= 0.16130276260913837\n",
      "-->epoch= 199 / 1000   loss= 0.16132500479595105\n",
      "-->epoch= 200 / 1000   loss= 0.16134201928885503\n",
      "-->epoch= 201 / 1000   loss= 0.16099288876448356\n",
      "-->epoch= 202 / 1000   loss= 0.161280182318081\n",
      "-->epoch= 203 / 1000   loss= 0.16135650991253367\n",
      "-->epoch= 204 / 1000   loss= 0.16129647218995846\n",
      "-->epoch= 205 / 1000   loss= 0.161149511921887\n",
      "-->epoch= 206 / 1000   loss= 0.1610280923031256\n",
      "-->epoch= 207 / 1000   loss= 0.16114444250697962\n",
      "-->epoch= 208 / 1000   loss= 0.16084117663005784\n",
      "-->epoch= 209 / 1000   loss= 0.16168746679390403\n",
      "-->epoch= 210 / 1000   loss= 0.1612412754879685\n",
      "-->epoch= 211 / 1000   loss= 0.16128863775026075\n",
      "-->epoch= 212 / 1000   loss= 0.16144702830213745\n",
      "-->epoch= 213 / 1000   loss= 0.1612946757131602\n",
      "-->epoch= 214 / 1000   loss= 0.16127462371583598\n",
      "-->epoch= 215 / 1000   loss= 0.16132922580951128\n",
      "-->epoch= 216 / 1000   loss= 0.1612380454994318\n",
      "-->epoch= 217 / 1000   loss= 0.1612333312977797\n",
      "-->epoch= 218 / 1000   loss= 0.16114114259957124\n",
      "-->epoch= 219 / 1000   loss= 0.16129662416274024\n",
      "-->epoch= 220 / 1000   loss= 0.16131027234861225\n",
      "-->epoch= 221 / 1000   loss= 0.16120365161301656\n",
      "-->epoch= 222 / 1000   loss= 0.16139185281866847\n",
      "-->epoch= 223 / 1000   loss= 0.16146270043068464\n",
      "-->epoch= 224 / 1000   loss= 0.16116626158901584\n",
      "-->epoch= 225 / 1000   loss= 0.160969672612683\n",
      "-->epoch= 226 / 1000   loss= 0.16109614684374365\n",
      "-->epoch= 227 / 1000   loss= 0.16137991561248322\n",
      "-->epoch= 228 / 1000   loss= 0.1611065213597519\n",
      "-->epoch= 229 / 1000   loss= 0.161240763279955\n",
      "-->epoch= 230 / 1000   loss= 0.1614226375718308\n",
      "-->epoch= 231 / 1000   loss= 0.1613948300446529\n",
      "-->epoch= 232 / 1000   loss= 0.16107949536783445\n",
      "-->epoch= 233 / 1000   loss= 0.16140153898021242\n",
      "-->epoch= 234 / 1000   loss= 0.16102951795078962\n",
      "-->epoch= 235 / 1000   loss= 0.16138205521400925\n",
      "-->epoch= 236 / 1000   loss= 0.16134216416726208\n",
      "-->epoch= 237 / 1000   loss= 0.16135424531209297\n",
      "-->epoch= 238 / 1000   loss= 0.1613847952316762\n",
      "-->epoch= 239 / 1000   loss= 0.16139855692599483\n",
      "-->epoch= 240 / 1000   loss= 0.16100777851734577\n",
      "-->epoch= 241 / 1000   loss= 0.16137375989176483\n",
      "-->epoch= 242 / 1000   loss= 0.16124630168895032\n",
      "-->epoch= 243 / 1000   loss= 0.16143775781839081\n",
      "-->epoch= 244 / 1000   loss= 0.16115896220037437\n",
      "-->epoch= 245 / 1000   loss= 0.16132155919155075\n",
      "-->epoch= 246 / 1000   loss= 0.16133953955968355\n",
      "-->epoch= 247 / 1000   loss= 0.1612099971845686\n",
      "-->epoch= 248 / 1000   loss= 0.1611386743636137\n",
      "-->epoch= 249 / 1000   loss= 0.16131438372938703\n",
      "-->epoch= 250 / 1000   loss= 0.16089210108080598\n",
      "-->epoch= 251 / 1000   loss= 0.16134679728492737\n",
      "-->epoch= 252 / 1000   loss= 0.16144486727437468\n",
      "-->epoch= 253 / 1000   loss= 0.16127482179071637\n",
      "-->epoch= 254 / 1000   loss= 0.1613394896685463\n",
      "-->epoch= 255 / 1000   loss= 0.1612360079741318\n",
      "-->epoch= 256 / 1000   loss= 0.1611593809556206\n",
      "-->epoch= 257 / 1000   loss= 0.16142266844832887\n",
      "-->epoch= 258 / 1000   loss= 0.16134324698989785\n",
      "-->epoch= 259 / 1000   loss= 0.16143244488050068\n",
      "-->epoch= 260 / 1000   loss= 0.1613967180484592\n",
      "-->epoch= 261 / 1000   loss= 0.1610634303998709\n",
      "-->epoch= 262 / 1000   loss= 0.16086689776561608\n",
      "-->epoch= 263 / 1000   loss= 0.1614294652093578\n",
      "-->epoch= 264 / 1000   loss= 0.16129313952632315\n",
      "-->epoch= 265 / 1000   loss= 0.16090796064903584\n",
      "-->epoch= 266 / 1000   loss= 0.16113357659732486\n",
      "-->epoch= 267 / 1000   loss= 0.16086597648605333\n",
      "-->epoch= 268 / 1000   loss= 0.16076471574999576\n",
      "-->epoch= 269 / 1000   loss= 0.16077016907104477\n",
      "-->epoch= 270 / 1000   loss= 0.16116607473961295\n",
      "-->epoch= 271 / 1000   loss= 0.1610884402633801\n",
      "-->epoch= 272 / 1000   loss= 0.16128786134989329\n",
      "-->epoch= 273 / 1000   loss= 0.16134810682093517\n",
      "-->epoch= 274 / 1000   loss= 0.16119044037672942\n",
      "-->epoch= 275 / 1000   loss= 0.16126594334561398\n",
      "-->epoch= 276 / 1000   loss= 0.1611380185588679\n",
      "-->epoch= 277 / 1000   loss= 0.16121794316539187\n",
      "-->epoch= 278 / 1000   loss= 0.16126128080117244\n",
      "-->epoch= 279 / 1000   loss= 0.16129012588539085\n",
      "-->epoch= 280 / 1000   loss= 0.16125643109958182\n",
      "-->epoch= 281 / 1000   loss= 0.1612285842972028\n",
      "-->epoch= 282 / 1000   loss= 0.16122782895506862\n",
      "-->epoch= 283 / 1000   loss= 0.1611277613003504\n",
      "-->epoch= 284 / 1000   loss= 0.16101544570366472\n",
      "-->epoch= 285 / 1000   loss= 0.16119463027269357\n",
      "-->epoch= 286 / 1000   loss= 0.16098418977402326\n",
      "-->epoch= 287 / 1000   loss= 0.16134947274839379\n",
      "-->epoch= 288 / 1000   loss= 0.16128869339902455\n",
      "-->epoch= 289 / 1000   loss= 0.16134971626883232\n",
      "-->epoch= 290 / 1000   loss= 0.1610764143382321\n",
      "-->epoch= 291 / 1000   loss= 0.16131197591887073\n",
      "-->epoch= 292 / 1000   loss= 0.1610737259620472\n",
      "-->epoch= 293 / 1000   loss= 0.16155511205702106\n",
      "-->epoch= 294 / 1000   loss= 0.16125738695377836\n",
      "-->epoch= 295 / 1000   loss= 0.1613640630275824\n",
      "-->epoch= 296 / 1000   loss= 0.16124317445466885\n",
      "-->epoch= 297 / 1000   loss= 0.1611007437290251\n",
      "-->epoch= 298 / 1000   loss= 0.16127023025157194\n",
      "-->epoch= 299 / 1000   loss= 0.16046253377375952\n",
      "-->epoch= 300 / 1000   loss= 0.16132616571926384\n"
     ]
    }
   ],
   "source": [
    "learningRate = 0.01\n",
    "net = createNetwork()\n",
    "data = readDatasetFromCSV(trainFileName)\n",
    "lossPlot=[]\n",
    "for it in range(300):\n",
    "    shufData, shufOutput = shuffleData(data)\n",
    "    sumLoss=0.0\n",
    "    for i in range(len(shufData)):\n",
    "        observedOutput, loss, target, nt = runForwardPropogation(net, shufData[i], shufOutput[i])\n",
    "        sumLoss+=loss\n",
    "        net = runBackwardPropogation(nt, target, shufData[i], learningRate)\n",
    "    sumLoss = sumLoss/len(shufData)\n",
    "    print(\"-->epoch=\", it+1, \"/ 1000   loss=\", sumLoss)\n",
    "#     for layer in net:\n",
    "#         print(layer)\n",
    "#     print(\"\\n\\n\")\n",
    "#     learningRate-=0.0005\n",
    "    lossPlot.append(sumLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Weights\n",
      "\n",
      "[-0.2847599330684195, 1.335822619026521]\n",
      "[0.2582987650805415, -1.2783045525056005]\n",
      "[-0.2767596766293208, 1.210084594239179]\n",
      "\n",
      "\n",
      "[1.3172475224971159, -2.798667726815434, 0.7548311671365965]\n",
      "[-1.2609559091035076, 2.01738717448787, -0.9017417770622949]\n",
      "[-1.4595938187037076, 1.9600776081857638, -0.6619649291966271]\n",
      "\n",
      "\n",
      "[4.161930613921799, -1.445046243477434, -1.341888925133302]\n",
      "[-2.914143021080107, 2.381893110930764, 2.424589560708355]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Learned Weights\\n\")\n",
    "for layer in net:\n",
    "#     print(layer)\n",
    "    for neuron in layer:\n",
    "        print(neuron['w'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j=[]\n",
    "for i in range(300):\n",
    "    j.append(i)\n",
    "plt.plot(np.asarray(j), np.asarray(lossPlot))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "   Total in test set: 1250\n",
      "   Correctly predicted: 1097\n",
      "   Accuracy:  87.76 %\n",
      "   Precision:  0.8761904761904762\n",
      "   Recall:  0.8803827751196173\n",
      "   F1-Score:  0.8782816229116945\n"
     ]
    }
   ],
   "source": [
    "### Running on test data\n",
    "testData = readDatasetFromCSV(testFileName)\n",
    "testInput=[]\n",
    "testOutput=[]\n",
    "for row in testData:\n",
    "    testInput.append(row[1:])\n",
    "    testOutput.append(row[0])\n",
    "truePositive=0\n",
    "trueNegative=0\n",
    "falsePositive=0\n",
    "falseNegative=0\n",
    "for i in range(len(testInput)):\n",
    "    o, l, tar, netw = runForwardPropogation(net, testInput[i], testOutput[i])\n",
    "#     print(o, testOutput[i])\n",
    "    if(testOutput[i]==0):\n",
    "        if (o[0]>o[1]):\n",
    "            trueNegative +=1\n",
    "        else:\n",
    "            falseNegative+=1\n",
    "    elif(testOutput[i]==1):\n",
    "        if (o[0]<o[1]):\n",
    "            truePositive +=1\n",
    "        else:\n",
    "            falsePositive+=1\n",
    "accuracy = ((truePositive + trueNegative) / len(testInput))*100\n",
    "precision=(truePositive)/(truePositive+falsePositive)\n",
    "recall=(truePositive)/(truePositive+falseNegative)\n",
    "f1score=2*((precision*recall)/(precision+recall))\n",
    "print(\"Test Metrics:\")\n",
    "print(\"   Total in test set:\", len(testInput))\n",
    "print(\"   Correctly predicted:\", truePositive+trueNegative)\n",
    "print(\"   Accuracy: \", accuracy,\"%\")\n",
    "print(\"   Precision: \", precision)\n",
    "print(\"   Recall: \", recall)\n",
    "print(\"   F1-Score: \", f1score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
